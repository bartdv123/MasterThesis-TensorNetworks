You'll be a professional julia coder and personal assistant working with me on making tensor network approximation codes in the julia languege. First, i'll give you all the source code to different packages inside of the Tenet.jl tensor network package and the EinExprs.jl contraction package. After you've read through the source code you'll be able to help me with debugging because you fully understand how the packages work.

module structure ofTenet

import EinExprs: inds

include("Helpers.jl")

include("Tensor.jl")
export Tensor, contract, dim, expand

include("Numerics.jl")

include("TensorNetwork.jl")
export TensorNetwork, tensors, arrays, select, slice!
export contract, contract!

include("Transformations.jl")
export transform, transform!

# reexports from EinExprs
export einexpr, inds

end


helpers.jl:


using Base.Iterators: Cycle, cycle

mutable struct RingPeek{Itr<:Cycle}
    const it::Itr
    base::Any
end

ringpeek(itr) = RingPeek(cycle(itr), nothing)
ringpeek(itr::Itr) where {Itr<:Cycle} = RingPeek(itr, nothing)

period(itr::Cycle) = length(itr.xs)
Base.IteratorSize(::Type{RingPeek{Itr}}) where {Itr} = Base.HasLength()
Base.length(itr::RingPeek{Itr}) where {Itr} = period(itr.it)

Base.IteratorEltype(::Type{RingPeek{Itr}}) where {Itr} = Base.IteratorEltype(Itr)
Base.eltype(::Type{RingPeek{Itr}}) where {Itr} = Tuple{eltype(Itr),eltype(Itr)}

Base.isdone(it::RingPeek, state) = it.base == state

function Base.iterate(it::RingPeek{Itr}) where {Itr}
    x, state = iterate(it.it)
    peeked, nextstate = iterate(it.it, state)
    it.base = state
    ((x, peeked), state)
end

function Base.iterate(it::RingPeek{Itr}, state) where {Itr}
    x, newstate = iterate(it.it, state)
    peeked, _ = iterate(it.it, newstate)

    newstate == it.base && return nothing

    ((x, peeked), newstate)
end

const NUM_UNICODE_LETTERS = VERSION >= v"1.9" ? 136104 : 131756

"""
    letter(i)

Return `i`-th printable Unicode letter.

# Examples

```jldoctest; setup = :(letter = Tenet.letter)
julia> letter(1)
:A

julia> letter(27)
:a

julia> letter(249)
:ƃ

julia> letter(20204)
:櫛
```
"""
letter(i) =
    Iterators.drop(Iterators.filter(isletter, Iterators.map(Char, 1:2^21-1)), i - 1) |> iterate |> first |> Symbol

# NOTE from https://stackoverflow.com/q/54652787
function nonunique(x)
    uniqueindexes = indexin(unique(x), x)
    nonuniqueindexes = setdiff(1:length(x), uniqueindexes)
    unique(x[nonuniqueindexes])
end


numerics.jl:

using OMEinsum
using LinearAlgebra
using UUIDs: uuid4
using SparseArrays

# TODO test array container typevar on output
for op in [
    :+,
    :-,
    :*,
    :/,
    :\,
    :^,
    :÷,
    :fld,
    :cld,
    :mod,
    :%,
    :fldmod,
    :fld1,
    :mod1,
    :fldmod1,
    ://,
    :gcd,
    :lcm,
    :gcdx,
    :widemul,
]
    @eval Base.$op(a::Tensor{A,0}, b::Tensor{B,0}) where {A,B} = broadcast($op, a, b)
end

# NOTE used for marking non-differentiability
# NOTE use `String[...]` code instead of `map` or broadcasting to set eltype in empty cases
__omeinsum_sym2str(x) = String[string(i) for i in x]

"""
    contract(a::Tensor[, b::Tensor]; dims=nonunique([inds(a)..., inds(b)...]))

Perform tensor contraction operation.
"""
function contract(a::Tensor, b::Tensor; dims = (∩(inds(a), inds(b))))
    ia = inds(a) |> collect
    ib = inds(b) |> collect
    i = ∩(dims, ia, ib)

    ic = setdiff(ia ∪ ib, i isa Base.AbstractVecOrTuple ? i : (i,))::Vector{Symbol}

    _ia = __omeinsum_sym2str(ia)
    _ib = __omeinsum_sym2str(ib)
    _ic = __omeinsum_sym2str(ic)

    data = EinCode((_ia, _ib), _ic)(parent(a), parent(b))

    return Tensor(data, ic)
end

function contract(a::Tensor; dims = nonunique(inds(a)))
    ia = inds(a)
    i = ∩(dims, ia)

    ic = setdiff(ia, i isa Base.AbstractVecOrTuple ? i : (i,))

    data = EinCode((String.(ia),), String.(ic))(parent(a))

    return Tensor(data, ic)
end

contract(a::Union{T,AbstractArray{T,0}}, b::Tensor{T}) where {T} = contract(Tensor(a), b)
contract(a::Tensor{T}, b::Union{T,AbstractArray{T,0}}) where {T} = contract(a, Tensor(b))
contract(a::AbstractArray{<:Any,0}, b::AbstractArray{<:Any,0}) = contract(Tensor(a), Tensor(b)) |> only
contract(a::Number, b::Number) = contract(fill(a), fill(b))
contract(tensors::Tensor...; kwargs...) = reduce((x, y) -> contract(x, y; kwargs...), tensors)

"""
    *(::Tensor, ::Tensor)

Alias for [`contract`](@ref).
"""
Base.:*(a::Tensor, b::Tensor) = contract(a, b)
Base.:*(a::T, b::Number) where {T<:Tensor} = T(parent(a) * b, inds(a))
Base.:*(a::Number, b::T) where {T<:Tensor} = T(a * parent(b), inds(b))

function factorinds(tensor, left_inds, right_inds)
    isdisjoint(left_inds, right_inds) ||
        throw(ArgumentError("left ($left_inds) and right $(right_inds) indices must be disjoint"))

    left_inds, right_inds =
        isempty(left_inds) ? (setdiff(inds(tensor), right_inds), right_inds) :
        isempty(right_inds) ? (left_inds, setdiff(inds(tensor), left_inds)) :
        throw(ArgumentError("cannot set both left and right indices"))

    all(!isempty, (left_inds, right_inds)) || throw(ArgumentError("no right-indices left in factorization"))
    all(∈(inds(tensor)), left_inds ∪ right_inds) || throw(ArgumentError("indices must be in $(inds(tensor))"))

    return left_inds, right_inds
end

LinearAlgebra.svd(t::Tensor{<:Any,2}; kwargs...) = Base.@invoke svd(t::Tensor; left_inds = (first(inds(t)),), kwargs...)

"""
    LinearAlgebra.svd(tensor::Tensor; left_inds, right_inds, virtualind, kwargs...)

Perform SVD factorization on a tensor.

# Keyword arguments

  - `left_inds`: left indices to be used in the SVD factorization. Defaults to all indices of `t` except `right_inds`.
  - `right_inds`: right indices to be used in the SVD factorization. Defaults to all indices of `t` except `left_inds`.
  - `virtualind`: name of the virtual bond. Defaults to a random `Symbol`.
"""
function LinearAlgebra.svd(tensor::Tensor; left_inds = (), right_inds = (), virtualind = Symbol(uuid4()), kwargs...)
    left_inds, right_inds = factorinds(tensor, left_inds, right_inds)

    virtualind ∉ inds(tensor) ||
        throw(ArgumentError("new virtual bond name ($virtualind) cannot be already be present"))

    # permute array
    left_sizes = map(Base.Fix1(size, tensor), left_inds)
    right_sizes = map(Base.Fix1(size, tensor), right_inds)
    tensor = permutedims(tensor, [left_inds..., right_inds...])
    data = reshape(parent(tensor), prod(left_sizes), prod(right_sizes))

    # compute SVD
    U, s, V = svd(data; kwargs...)

    # tensorify results
    U = Tensor(reshape(U, left_sizes..., size(U, 2)), [left_inds..., virtualind])
    s = Tensor(s, [virtualind])
    Vt = Tensor(reshape(V, right_sizes..., size(V, 2)), [right_inds..., virtualind])

    return U, s, Vt
end

LinearAlgebra.qr(t::Tensor{<:Any,2}; kwargs...) = Base.@invoke qr(t::Tensor; left_inds = (first(inds(t)),), kwargs...)

"""
    LinearAlgebra.qr(tensor::Tensor; left_inds, right_inds, virtualind, kwargs...)

Perform QR factorization on a tensor.

# Keyword arguments

  - `left_inds`: left indices to be used in the QR factorization. Defaults to all indices of `t` except `right_inds`.
  - `right_inds`: right indices to be used in the QR factorization. Defaults to all indices of `t` except `left_inds`.
  - `virtualind`: name of the virtual bond. Defaults to a random `Symbol`.
"""
function LinearAlgebra.qr(
    tensor::Tensor;
    left_inds = (),
    right_inds = (),
    virtualind::Symbol = Symbol(uuid4()),
    kwargs...,
)
    left_inds, right_inds = factorinds(tensor, left_inds, right_inds)

    virtualind ∉ inds(tensor) ||
        throw(ArgumentError("new virtual bond name ($virtualind) cannot be already be present"))

    # permute array
    left_sizes = map(Base.Fix1(size, tensor), left_inds)
    right_sizes = map(Base.Fix1(size, tensor), right_inds)
    tensor = permutedims(tensor, [left_inds..., right_inds...])
    data = reshape(parent(tensor), prod(left_sizes), prod(right_sizes))

    # compute QR
    F = qr(data; kwargs...)
    Q, R = Matrix(F.Q), Matrix(F.R)

    # tensorify results
    Q = Tensor(reshape(Q, left_sizes..., size(Q, 2)), [left_inds..., virtualind])
    R = Tensor(reshape(R, size(R, 1), right_sizes...), [virtualind, right_inds...])

    return Q, R
end

LinearAlgebra.lu(t::Tensor{<:Any,2}; kwargs...) = Base.@invoke lu(t::Tensor; left_inds = (first(inds(t)),), kwargs...)

"""
    LinearAlgebra.lu(tensor::Tensor; left_inds, right_inds, virtualind, kwargs...)

Perform LU factorization on a tensor.

# Keyword arguments

  - `left_inds`: left indices to be used in the LU factorization. Defaults to all indices of `t` except `right_inds`.
  - `right_inds`: right indices to be used in the LU factorization. Defaults to all indices of `t` except `left_inds`.
  - `virtualind`: name of the virtual bond. Defaults to a random `Symbol`.
"""
function LinearAlgebra.lu(
    tensor::Tensor;
    left_inds = (),
    right_inds = (),
    virtualind = [Symbol(uuid4()), Symbol(uuid4())],
    kwargs...,
)
    left_inds, right_inds = factorinds(tensor, left_inds, right_inds)

    i_pl, i_lu = virtualind
    i_pl ∉ inds(tensor) || throw(ArgumentError("new virtual bond name ($i_pl) cannot be already be present"))
    i_lu ∉ inds(tensor) || throw(ArgumentError("new virtual bond name ($i_lu) cannot be already be present"))

    # permute array
    left_sizes = map(Base.Fix1(size, tensor), left_inds)
    right_sizes = map(Base.Fix1(size, tensor), right_inds)
    tensor = permutedims(tensor, [left_inds..., right_inds...])
    data = reshape(parent(tensor), prod(left_sizes), prod(right_sizes))

    # compute LU
    info = lu(data; kwargs...)
    L = info.L
    U = info.U

    permutator = info.p
    P = sparse(permutator, 1:length(permutator), fill(true, length(permutator)))

    L = Tensor(L, [i_pl, i_lu])
    U = Tensor(reshape(U, size(U, 1), right_sizes...), [i_lu, right_inds...])
    P = Tensor(reshape(P, left_sizes..., size(L, 1)), [left_inds..., i_pl])

    return L, U, P
end


Tensor.jl:

using Base: @propagate_inbounds
using Base.Broadcast: Broadcasted, ArrayStyle
using ImmutableArrays

struct Tensor{T,N,A<:AbstractArray{T,N}} <: AbstractArray{T,N}
    data::A
    inds::ImmutableVector{Symbol,Vector{Symbol}}

    function Tensor{T,N,A}(data::A, inds::AbstractVector) where {T,N,A<:AbstractArray{T,N}}
        length(inds) == N ||
            throw(ArgumentError("ndims(data) [$(ndims(data))] must be equal to length(inds) [$(length(inds))]"))
        all(i -> allequal(Iterators.map(dim -> size(data, dim), findall(==(i), inds))), nonunique(collect(inds))) ||
            throw(DimensionMismatch("nonuniform size of repeated indices"))

        new{T,N,A}(data, ImmutableArray(inds))
    end
end

Tensor(data::A, inds::AbstractVector{Symbol}) where {T,N,A<:AbstractArray{T,N}} = Tensor{T,N,A}(data, inds)
Tensor(data::A, inds::NTuple{N,Symbol}) where {T,N,A<:AbstractArray{T,N}} = Tensor{T,N,A}(data, collect(inds))

Tensor(data::AbstractArray{T,0}) where {T} = Tensor(data, Symbol[])
Tensor(data::Number) = Tensor(fill(data))

inds(t::Tensor) = t.inds

function Base.copy(t::Tensor{T,N,<:SubArray{T,N}}) where {T,N}
    data = copy(t.data)
    inds = t.inds
    return Tensor(data, inds)
end

# TODO pass new inds
function Base.similar(t::Tensor{_,N}, ::Type{T}) where {_,T,N}
    if N == 0
        return Tensor(similar(parent(t), T), Symbol[])
    else
        similar(t, T, size(t)...)
    end
end

Base.similar(t::Tensor, T::Type, dims::Int64...; inds = inds(t)) = Tensor(similar(parent(t), T, dims), inds)

function __find_index_permutation(a, b)
    inds_b = collect(Union{Missing,Symbol}, b)

    Iterators.map(a) do label
        i = findfirst(isequal(label), inds_b)

        # mark element as used
        inds_b[i] = missing

        i
    end |> collect
end

Base.:(==)(a::AbstractArray, b::Tensor) = isequal(b, a)
Base.:(==)(a::Tensor, b::AbstractArray) = isequal(a, b)
Base.:(==)(a::Tensor, b::Tensor) = isequal(a, b)
Base.isequal(a::AbstractArray, b::Tensor) = false
Base.isequal(a::Tensor, b::AbstractArray) = false
function Base.isequal(a::Tensor, b::Tensor)
    issetequal(inds(a), inds(b)) || return false
    perm = __find_index_permutation(inds(a), inds(b))
    return all(eachindex(IndexCartesian(), a)) do i
        j = CartesianIndex(Tuple(permute!(collect(Tuple(i)), invperm(perm))))
        isequal(a[i], b[j])
    end
end

Base.isapprox(a::AbstractArray, b::Tensor) = false
Base.isapprox(a::Tensor, b::AbstractArray) = false
function Base.isapprox(a::Tensor, b::Tensor)
    issetequal(inds(a), inds(b)) || return false
    perm = __find_index_permutation(inds(a), inds(b))
    return all(eachindex(IndexCartesian(), a)) do i
        j = CartesianIndex(Tuple(permute!(collect(Tuple(i)), invperm(perm))))
        isapprox(a[i], b[j])
    end
end

# NOTE: `replace` does not currenly support cyclic replacements
Base.replace(t::Tensor, old_new::Pair{Symbol,Symbol}...) = Tensor(parent(t), replace(inds(t), old_new...))

Base.parent(t::Tensor) = t.data
parenttype(::Type{Tensor{T,N,A}}) where {T,N,A} = A

dim(t::Tensor, i::Number) = i
dim(t::Tensor, i::Symbol) = findall(==(i), inds(t)) |> first

# Iteration interface
Base.IteratorSize(T::Type{Tensor}) = Iterators.IteratorSize(parenttype(T))
Base.IteratorEltype(T::Type{Tensor}) = Iterators.IteratorEltype(parenttype(T))

Base.isdone(t::Tensor) = (Base.isdone ∘ parent)(t)
Base.isdone(t::Tensor, state) = (Base.isdone ∘ parent)(t)

# Indexing interface
Base.IndexStyle(T::Type{<:Tensor}) = IndexStyle(parenttype(T))

@propagate_inbounds Base.getindex(t::Tensor, i...) = getindex(parent(t), i...)
@propagate_inbounds function Base.getindex(t::Tensor; i...)
    length(i) == 0 && return (getindex ∘ parent)(t)
    return getindex(t, [get(i, label, Colon()) for label in inds(t)]...)
end

@propagate_inbounds Base.setindex!(t::Tensor, v, i...) = setindex!(parent(t), v, i...)
@propagate_inbounds function Base.setindex!(t::Tensor, v; i...)
    length(i) == 0 && return setindex!(parent(t), v)
    return setindex!(t, v, [get(i, label, Colon()) for label in inds(t)]...)
end

Base.firstindex(t::Tensor) = firstindex(parent(t))
Base.lastindex(t::Tensor) = lastindex(parent(t))

# AbstractArray interface
"""
    Base.size(::Tensor[, i])

Return the size of the underlying array or the dimension `i` (specified by `Symbol` or `Integer`).
"""
Base.size(t::Tensor) = size(parent(t))
Base.size(t::Tensor, i) = size(parent(t), dim(t, i))

Base.length(t::Tensor) = length(parent(t))

Base.axes(t::Tensor) = axes(parent(t))
Base.axes(t::Tensor, d) = axes(parent(t), dim(t, d))

# StridedArrays interface
Base.strides(t::Tensor) = strides(parent(t))
Base.stride(t::Tensor, i::Symbol) = stride(parent(t), dim(t, i))

Base.unsafe_convert(::Type{Ptr{T}}, t::Tensor{T}) where {T} = Base.unsafe_convert(Ptr{T}, parent(t))

Base.elsize(T::Type{<:Tensor}) = elsize(parenttype(T))

# Broadcasting
Base.BroadcastStyle(::Type{T}) where {T<:Tensor} = ArrayStyle{T}()

function Base.similar(bc::Broadcasted{ArrayStyle{Tensor{T,N,A}}}, ::Type{ElType}) where {T,N,A,ElType}
    # NOTE already checked if dimension mismatch
    # TODO throw on label mismatch?
    tensor = first(arg for arg in bc.args if arg isa Tensor{T,N,A})
    similar(tensor, ElType)
end

Base.selectdim(t::Tensor, d::Integer, i) = Tensor(selectdim(parent(t), d, i), inds(t))
function Base.selectdim(t::Tensor, d::Integer, i::Integer)
    data = selectdim(parent(t), d, i)
    indices = [label for (i, label) in enumerate(inds(t)) if i != d]
    Tensor(data, indices)
end

Base.selectdim(t::Tensor, d::Symbol, i) = selectdim(t, dim(t, d), i)

Base.permutedims(t::Tensor, perm) = Tensor(permutedims(parent(t), perm), getindex.((inds(t),), perm))
Base.permutedims!(dest::Tensor, src::Tensor, perm) = permutedims!(parent(dest), parent(src), perm)

function Base.permutedims(t::Tensor{T}, perm::Base.AbstractVecOrTuple{Symbol}) where {T}
    perm = map(i -> findfirst(==(i), inds(t)), perm)
    permutedims(t, perm)
end

Base.dropdims(t::Tensor; dims = tuple(findall(==(1), size(t))...)) =
    Tensor(dropdims(parent(t); dims), inds(t)[setdiff(1:ndims(t), dims)])

Base.view(t::Tensor, i...) =
    Tensor(view(parent(t), i...), [label for (label, j) in zip(inds(t), i) if !(j isa Integer)])

function Base.view(t::Tensor, inds::Pair{Symbol,<:Any}...)
    indices = map(Tenet.inds(t)) do ind
        i = findfirst(x -> x == ind, first.(inds))
        !isnothing(i) ? inds[i].second : Colon()
    end

    let data = view(parent(t), indices...),
        inds = [label for (index, label) in zip(indices, Tenet.inds(t)) if !(index isa Integer)]

        Tensor(data, inds)
    end
end

Base.adjoint(t::Tensor) = Tensor(conj(parent(t)), inds(t))

# NOTE: Maybe use transpose for lazy transposition ?
Base.transpose(t::Tensor{T,1,A}) where {T,A<:AbstractArray{T,1}} = permutedims(t, (1,))
Base.transpose(t::Tensor{T,2,A}) where {T,A<:AbstractArray{T,2}} = Tensor(transpose(parent(t)), reverse(inds(t)))

function expand(tensor::Tensor; label, axis = 1, size = 1, method = :zeros)
    array = parent(tensor)
    data =
        size == 1 ? reshape(array, Base.size(array)[1:axis-1]..., 1, Base.size(array)[axis:end]...) :
        method === :zeros ? __expand_zeros(array, axis, size) :
        method === :repeat ? __expand_repeat(array, axis, size) :
        # method === :identity ? __expand_identity(array, axis, size) :
        throw(ArgumentError("method \"$method\" is not valid"))

    inds = (Tenet.inds(tensor)[1:axis-1]..., label, Tenet.inds(tensor)[axis:end]...)

    return Tensor(data, inds)
end

function __expand_zeros(array, axis, size)
    new = zeros(eltype(array), Base.size(array)[1:axis-1]..., size, Base.size(array)[axis:end]...)

    view = selectdim(new, axis, 1)
    copy!(view, array)

    return new
end

__expand_repeat(array, axis, size) = repeat(
    reshape(array, Base.size(array)[1:axis-1]..., 1, Base.size(array)[axis:end]...),
    outer = (fill(1, axis - 1)..., size, fill(1, ndims(array) - axis + 1)...),
)



TensorNetworks.jl:

using Base: AbstractVecOrTuple
using Random
using EinExprs
using OMEinsum
using ValSplit

abstract type AbstractTensorNetwork end

"""
    TensorNetwork

Graph of interconnected tensors, representing a multilinear equation.
Graph vertices represent tensors and graph edges, tensor indices.
"""
struct TensorNetwork <: AbstractTensorNetwork
    indexmap::Dict{Symbol,Vector{Tensor}}
    tensormap::IdDict{Tensor,Vector{Symbol}}

    function TensorNetwork(tensors)
        tensormap = IdDict{Tensor,Vector{Symbol}}(tensor => inds(tensor) for tensor in tensors)

        indexmap = reduce(tensors; init = Dict{Symbol,Vector{Tensor}}()) do dict, tensor
            # TODO check for inconsistent dimensions?
            for index in inds(tensor)
                # TODO use lambda? `Tensor[]` might be reused
                push!(get!(dict, index, Tensor[]), tensor)
            end
            dict
        end

        new(indexmap, tensormap)
    end
end

TensorNetwork() = TensorNetwork(Tensor[])

"""
    copy(tn::TensorNetwork)

Return a shallow copy of a [`TensorNetwork`](@ref).
"""
Base.copy(tn::T) where {T<:AbstractTensorNetwork} = TensorNetwork(tensors(tn))

Base.summary(io::IO, tn::AbstractTensorNetwork) = print(io, "$(length(tn.tensormap))-tensors $(typeof(tn))")
Base.show(io::IO, tn::AbstractTensorNetwork) =
    print(io, "$(typeof(tn))(#tensors=$(length(tn.tensormap)), #inds=$(length(tn.indexmap)))")

"""
    tensors(tn::AbstractTensorNetwork)

Return a list of the `Tensor`s in the [`TensorNetwork`](@ref).

# Implementation details

  - As the tensors of a [`TensorNetwork`](@ref) are stored as keys of the `.tensormap` dictionary and it uses `objectid` as hash, order is not stable so it sorts for repeated evaluations.
"""
tensors(tn::AbstractTensorNetwork) = sort!(collect(keys(tn.tensormap)), by = inds)
arrays(tn::AbstractTensorNetwork) = parent.(tensors(tn))

Base.collect(tn::AbstractTensorNetwork) = tensors(tn)

"""
    inds(tn::AbstractTensorNetwork, set = :all)

Return the names of the indices in the [`TensorNetwork`](@ref).

# Keyword Arguments

  - `set`

      + `:all` (default) All indices.
      + `:open` Indices only mentioned in one tensor.
      + `:inner` Indices mentioned at least twice.
      + `:hyper` Indices mentioned at least in three tensors.
"""
Tenet.inds(tn::AbstractTensorNetwork; set::Symbol = :all, kwargs...) = inds(tn, set; kwargs...)
@valsplit 2 Tenet.inds(tn::AbstractTensorNetwork, set::Symbol, args...) = throw(MethodError(inds, "unknown set=$set"))

function Tenet.inds(tn::AbstractTensorNetwork, ::Val{:all})
    collect(keys(tn.indexmap))
end

function Tenet.inds(tn::AbstractTensorNetwork, ::Val{:open})
    map(first, Iterators.filter(((_, v),) -> length(v) == 1, tn.indexmap))
end

function Tenet.inds(tn::AbstractTensorNetwork, ::Val{:inner})
    map(first, Iterators.filter(((_, v),) -> length(v) >= 2, tn.indexmap))
end

function Tenet.inds(tn::AbstractTensorNetwork, ::Val{:hyper})
    map(first, Iterators.filter(((_, v),) -> length(v) >= 3, tn.indexmap))
end

"""
    size(tn::AbstractTensorNetwork)
    size(tn::AbstractTensorNetwork, index)

Return a mapping from indices to their dimensionalities.

If `index` is set, return the dimensionality of `index`. This is equivalent to `size(tn)[index]`.
"""
Base.size(tn::AbstractTensorNetwork) = Dict{Symbol,Int}(index => size(tn, index) for index in keys(tn.indexmap))
Base.size(tn::AbstractTensorNetwork, index::Symbol) = size(first(tn.indexmap[index]), index)

Base.eltype(tn::AbstractTensorNetwork) = promote_type(eltype.(tensors(tn))...)

"""
    push!(tn::AbstractTensorNetwork, tensor::Tensor)

Add a new `tensor` to the Tensor Network.

See also: [`append!`](@ref), [`pop!`](@ref).
"""
function Base.push!(tn::AbstractTensorNetwork, tensor::Tensor)
    tensor ∈ keys(tn.tensormap) && return tn

    # check index sizes
    for i in Iterators.filter(i -> size(tn, i) != size(tensor, i), inds(tensor) ∩ inds(tn))
        throw(DimensionMismatch("size(tensor,$i)=$(size(tensor,i)) but should be equal to size(tn,$i)=$(size(tn,i))"))
    end

    tn.tensormap[tensor] = collect(inds(tensor))
    for index in unique(inds(tensor))
        push!(get!(tn.indexmap, index, Tensor[]), tensor)
    end

    return tn
end

"""
    append!(tn::AbstractTensorNetwork, tensors::AbstractVecOrTuple{<:Tensor})

Add a list of tensors to a `TensorNetwork`.

See also: [`push!`](@ref), [`merge!`](@ref).
"""
Base.append!(tn::AbstractTensorNetwork, tensors) = (foreach(Base.Fix1(push!, tn), tensors); tn)

"""
    merge!(self::AbstractTensorNetwork, others::AbstractTensorNetwork...)
    merge(self::AbstractTensorNetwork, others::AbstractTensorNetwork...)

Fuse various [`TensorNetwork`](@ref)s into one.

See also: [`append!`](@ref).
"""
Base.merge!(self::AbstractTensorNetwork, other::AbstractTensorNetwork) = append!(self, tensors(other))
Base.merge!(self::AbstractTensorNetwork, others::AbstractTensorNetwork...) = foldl(merge!, others; init = self)
Base.merge(self::AbstractTensorNetwork, others::AbstractTensorNetwork...) = merge!(copy(self), others...)

"""
    pop!(tn::AbstractTensorNetwork, tensor::Tensor)
    pop!(tn::AbstractTensorNetwork, i::Union{Symbol,AbstractVecOrTuple{Symbol}})

Remove a tensor from the Tensor Network and returns it. If a `Tensor` is passed, then the first tensor satisfies _egality_ (i.e. `≡` or `===`) will be removed.
If a `Symbol` or a list of `Symbol`s is passed, then remove and return the tensors that contain all the indices.

See also: [`push!`](@ref), [`delete!`](@ref).
"""
Base.pop!(tn::AbstractTensorNetwork, tensor::Tensor) = (delete!(tn, tensor); tensor)
Base.pop!(tn::AbstractTensorNetwork, i::Symbol) = pop!(tn, (i,))

function Base.pop!(tn::AbstractTensorNetwork, i::AbstractVecOrTuple{Symbol})::Vector{Tensor}
    tensors = select(tn, i)
    for tensor in tensors
        _ = pop!(tn, tensor)
    end

    return tensors
end

"""
    delete!(tn::AbstractTensorNetwork, x)

Like [`pop!`](@ref) but return the [`TensorNetwork`](@ref) instead.
"""
Base.delete!(tn::AbstractTensorNetwork, x) = (_ = pop!(tn, x); tn)

tryprune!(tn::AbstractTensorNetwork, i::Symbol) = (x = isempty(tn.indexmap[i]) && delete!(tn.indexmap, i); x)

function Base.delete!(tn::AbstractTensorNetwork, tensor::Tensor)
    for index in unique(inds(tensor))
        filter!(Base.Fix1(!==, tensor), tn.indexmap[index])
        tryprune!(tn, index)
    end
    delete!(tn.tensormap, tensor)

    return tn
end

"""
    replace!(tn::AbstractTensorNetwork, old => new...)
    replace(tn::AbstractTensorNetwork, old => new...)

Replace the element in `old` with the one in `new`. Depending on the types of `old` and `new`, the following behaviour is expected:

  - If `Symbol`s, it will correspond to a index renaming.
  - If `Tensor`s, first element that satisfies _egality_ (`≡` or `===`) will be replaced.
"""
Base.replace!(tn::AbstractTensorNetwork, old_new::Pair...) = replace!(tn, old_new)
function Base.replace!(tn::AbstractTensorNetwork, old_new::Base.AbstractVecOrTuple{Pair})
    for pair in old_new
        replace!(tn, pair)
    end
    return tn
end
Base.replace(tn::AbstractTensorNetwork, old_new::Pair...) = replace(tn, old_new)
Base.replace(tn::AbstractTensorNetwork, old_new) = replace!(copy(tn), old_new)

function Base.replace!(tn::AbstractTensorNetwork, pair::Pair{<:Tensor,<:Tensor})
    old_tensor, new_tensor = pair
    issetequal(inds(new_tensor), inds(old_tensor)) || throw(ArgumentError("replacing tensor indices don't match"))

    push!(tn, new_tensor)
    delete!(tn, old_tensor)

    return tn
end

function Base.replace!(tn::AbstractTensorNetwork, old_new::Pair{Symbol,Symbol}...)
    first.(old_new) ⊆ keys(tn.indexmap) ||
        throw(ArgumentError("set of old indices must be a subset of current indices"))
    isdisjoint(last.(old_new), keys(tn.indexmap)) ||
        throw(ArgumentError("set of new indices must be disjoint to current indices"))
    for pair in old_new
        replace!(tn, pair)
    end
    return tn
end

function Base.replace!(tn::AbstractTensorNetwork, old_new::Pair{Symbol,Symbol})
    old, new = old_new
    old ∈ keys(tn.indexmap) || throw(ArgumentError("index $old does not exist"))
    new ∉ keys(tn.indexmap) || throw(ArgumentError("index $new is already present"))

    # NOTE `copy` because collection underneath is mutated
    for tensor in copy(tn.indexmap[old])
        # NOTE do not `delete!` before `push!` as indices can be lost due to `tryprune!`
        push!(tn, replace(tensor, old_new))
        delete!(tn, tensor)
    end

    delete!(tn.indexmap, old)

    return tn
end

function Base.replace!(tn::AbstractTensorNetwork, old_new::Pair{<:Tensor,<:AbstractTensorNetwork})
    old, new = old_new
    issetequal(inds(new, set = :open), inds(old)) || throw(ArgumentError("indices don't match match"))

    # rename internal indices so there is no accidental hyperedge
    replace!(new, [index => Symbol(uuid4()) for index in filter(∈(inds(tn)), inds(new, set = :inner))]...)

    merge!(tn, new)
    delete!(tn, old)

    return tn
end

"""
    select(tn::AbstractTensorNetwork, i)

Return tensors whose indices match with the list of indices `i`.
"""
select(tn::AbstractTensorNetwork, i::Symbol) = copy(tn.indexmap[i])
select(tn::AbstractTensorNetwork, is::AbstractVecOrTuple{Symbol}) = select(⊆, tn, is)

function select(selector, tn::TensorNetwork, is::AbstractVecOrTuple{Symbol})
    filter(Base.Fix1(selector, is) ∘ inds, tn.indexmap[first(is)])
end

function Base.getindex(tn::TensorNetwork, is::Symbol...; mul::Int = 1)
    first(Iterators.drop(Iterators.filter(Base.Fix1(issetequal, is) ∘ inds, tn.indexmap[first(is)]), mul - 1))
end

"""
    in(tensor::Tensor, tn::AbstractTensorNetwork)
    in(index::Symbol, tn::AbstractTensorNetwork)

Return `true` if there is a `Tensor` in `tn` for which `==` evaluates to `true`.
This method is equivalent to `tensor ∈ tensors(tn)` code, but it's faster on large amount of tensors.
"""
Base.in(tensor::Tensor, tn::AbstractTensorNetwork) = tensor ∈ keys(tn.tensormap)
Base.in(index::Symbol, tn::AbstractTensorNetwork) = index ∈ keys(tn.indexmap)

"""
    slice!(tn::AbstractTensorNetwork, index::Symbol, i)

In-place projection of `index` on dimension `i`.

See also: [`selectdim`](@ref), [`view`](@ref).
"""
function slice!(tn::AbstractTensorNetwork, label::Symbol, i)
    for tensor in pop!(tn, label)
        push!(tn, selectdim(tensor, label, i))
    end

    return tn
end

"""
    selectdim(tn::AbstractTensorNetwork, index::Symbol, i)

Return a copy of the [`TensorNetwork`](@ref) where `index` has been projected to dimension `i`.

See also: [`view`](@ref), [`slice!`](@ref).
"""
Base.selectdim(tn::AbstractTensorNetwork, label::Symbol, i) = @view tn[label=>i]

"""
    view(tn::AbstractTensorNetwork, index => i...)

Return a copy of the [`TensorNetwork`](@ref) where each `index` has been projected to dimension `i`.
It is equivalent to a recursive call of [`selectdim`](@ref).

See also: [`selectdim`](@ref), [`slice!`](@ref).
"""
function Base.view(tn::AbstractTensorNetwork, slices::Pair{Symbol}...)
    tn = copy(tn)

    for (label, i) in slices
        slice!(tn, label, i)
    end

    return tn
end

"""
    rand(TensorNetwork, n::Integer, regularity::Integer; out = 0, dim = 2:9, seed = nothing, globalind = false)

Generate a random tensor network.

# Arguments

  - `n` Number of tensors.
  - `regularity` Average number of indices per tensor.
  - `out` Number of open indices.
  - `dim` Range of dimension sizes.
  - `seed` If not `nothing`, seed random generator with this value.
  - `globalind` Add a global 'broadcast' dimension to every tensor.
"""
function Base.rand(
    ::Type{TensorNetwork},
    n::Integer,
    regularity::Integer;
    out = 0,
    dim = 2:9,
    seed = nothing,
    globalind = false,
)
    !isnothing(seed) && Random.seed!(seed)

    inds = letter.(randperm(n * regularity ÷ 2 + out))
    size_dict = Dict(ind => rand(dim) for ind in inds)

    outer_inds = Iterators.take(inds, out) |> collect
    inner_inds = Iterators.drop(inds, out) |> collect

    candidate_inds =
        [outer_inds, Iterators.flatten(Iterators.repeated(inner_inds, 2))] |> Iterators.flatten |> collect |> shuffle

    inputs = map(x -> [x], Iterators.take(candidate_inds, n))

    for ind in Iterators.drop(candidate_inds, n)
        i = rand(1:n)
        while ind in inputs[i]
            i = rand(1:n)
        end

        push!(inputs[i], ind)
    end

    if globalind
        ninds = length(size_dict)
        ind = letter(ninds + 1)
        size_dict[ind] = rand(dim)
        push!(outer_inds, ind)
        push!.(inputs, (ind,))
    end

    tensors = Tensor[Tensor(rand([size_dict[ind] for ind in input]...), tuple(input...)) for input in inputs]
    TensorNetwork(tensors)
end

"""
    einexpr(tn::AbstractTensorNetwork; optimizer = EinExprs.Greedy, output = inds(tn, :open), kwargs...)

Search a contraction path for the given [`TensorNetwork`](@ref) and return it as a `EinExpr`.

# Keyword Arguments

  - `optimizer` Contraction path optimizer. Check [`EinExprs`](https://github.com/bsc-quantic/EinExprs.jl) documentation for more info.
  - `outputs` Indices that won't be contracted. Defaults to open indices.
  - `kwargs` Options to be passed to the optimizer.

See also: [`contract`](@ref).
"""
EinExprs.einexpr(tn::AbstractTensorNetwork; optimizer = Greedy, outputs = inds(tn, :open), kwargs...) = einexpr(
    optimizer,
    EinExpr(
        outputs,
        [EinExpr(inds(tensor), Dict(index => size(tensor, index) for index in inds(tensor))) for tensor in tensors(tn)],
    );
    kwargs...,
)

# TODO sequence of indices?
# TODO what if parallel neighbour indices?
"""
    contract!(tn::AbstractTensorNetwork, index)

In-place contraction of tensors connected to `index`.

See also: [`contract`](@ref).
"""
function contract!(tn::AbstractTensorNetwork, i)
    tensor = reduce(pop!(tn, i)) do acc, tensor
        contract(acc, tensor, dims = i)
    end

    push!(tn, tensor)
    return tn
end

"""
    contract(tn::AbstractTensorNetwork; kwargs...)

Contract a [`TensorNetwork`](@ref). The contraction order will be first computed by [`einexpr`](@ref).

The `kwargs` will be passed down to the [`einexpr`](@ref) function.

See also: [`einexpr`](@ref), [`contract!`](@ref).
"""
function contract(tn::AbstractTensorNetwork; path = einexpr(tn))
    # TODO does `first` work always?
    length(path.args) == 0 && return select(tn, inds(path)) |> first

    intermediates = map(subpath -> contract(tn; path = subpath), path.args)
    contract(intermediates...; dims = suminds(path))
end

contract!(t::Tensor, tn::AbstractTensorNetwork; kwargs...) = contract!(tn, t; kwargs...)
contract!(tn::AbstractTensorNetwork, t::Tensor; kwargs...) = (push!(tn, t); contract(tn; kwargs...))
contract(t::Tensor, tn::AbstractTensorNetwork; kwargs...) = contract(tn, t; kwargs...)
contract(tn::AbstractTensorNetwork, t::Tensor; kwargs...) = contract!(copy(tn), t; kwargs...)

struct TNSampler{T<:AbstractTensorNetwork} <: Random.Sampler{T}
    config::Dict{Symbol,Any}

    TNSampler{T}(; kwargs...) where {T} = new{T}(kwargs)
end

Base.eltype(::TNSampler{T}) where {T} = T

Base.getproperty(obj::TNSampler, name::Symbol) = name === :config ? getfield(obj, :config) : obj.config[name]
Base.get(obj::TNSampler, name, default) = get(obj.config, name, default)

Base.rand(T::Type{<:AbstractTensorNetwork}; kwargs...) = rand(Random.default_rng(), T; kwargs...)
Base.rand(rng::AbstractRNG, T::Type{<:AbstractTensorNetwork}; kwargs...) = rand(rng, TNSampler{T}(; kwargs...))


Transformations.jl:

using DeltaArrays
using EinExprs
using OMEinsum
using UUIDs: uuid4
using Tenet: parenttype
using Combinatorics: combinations

abstract type Transformation end

"""
    transform(tn::AbstractTensorNetwork, config::Transformation)
    transform(tn::AbstractTensorNetwork, configs)

Return a new [`TensorNetwork`](@ref) where some `Transformation` has been performed into it.

See also: [`transform!`](@ref).
"""
transform(tn::AbstractTensorNetwork, transformations) = (tn = deepcopy(tn); transform!(tn, transformations); return tn)

"""
    transform!(tn::AbstractTensorNetwork, config::Transformation)
    transform!(tn::AbstractTensorNetwork, configs)

In-place version of [`transform`](@ref).
"""
function transform! end

transform!(tn::AbstractTensorNetwork, transformation::Type{<:Transformation}; kwargs...) =
    transform!(tn, transformation(kwargs...))

function transform!(tn::AbstractTensorNetwork, transformations)
    for transformation in transformations
        transform!(tn, transformation)
    end
    return tn
end

"""
    HyperindConverter <: Transformation

Convert hyperindices to COPY-tensors, represented by `DeltaArray`s.
This transformation is always used by default when visualizing a `TensorNetwork` with `plot`.
"""
struct HyperindConverter <: Transformation end

function hyperflatten(tn::AbstractTensorNetwork)
    map(inds(tn, :hyper)) do hyperindex
        n = select(tn, hyperindex) |> length
        map(1:n) do i
            Symbol("$hyperindex$i")
        end => hyperindex
    end |> Dict
end

function transform!(tn::AbstractTensorNetwork, ::HyperindConverter)
    for (flatindices, hyperindex) in hyperflatten(tn)
        # insert COPY tensor
        array = DeltaArray{length(flatindices)}(ones(size(tn, hyperindex)))
        tensor = Tensor(array, flatindices)
        push!(tn, tensor)

        # replace hyperindex for new flat Indices
        # TODO move this part to `replace!`?
        tensors = pop!(tn, hyperindex)
        for (flatindex, tensor) in zip(flatindices, tensors)
            tensor = replace(tensor, hyperindex => flatindex)
            push!(tn, tensor)
        end
    end
end

"""
    DiagonalReduction <: Transformation

Reduce the dimension of a `Tensor` in a [`TensorNetwork`](@ref) when it has a pair of indices that fulfil a diagonal structure.

# Keyword Arguments

  - `atol` Absolute tolerance. Defaults to `1e-12`.
"""
Base.@kwdef struct DiagonalReduction <: Transformation
    atol::Float64 = 1e-12
end

function transform!(tn::AbstractTensorNetwork, config::DiagonalReduction)
    for tensor in filter(tensor -> !(parenttype(typeof(tensor)) <: DeltaArray), tensors(tn))
        diaginds = find_diag_axes(tensor, atol = config.atol)
        isempty(diaginds) && continue

        transformed_tensor = reduce(diaginds; init = (; target = tensor, copies = Tensor[])) do (target, copies), inds
            N = length(inds)

            # insert COPY tensor
            new_index = Symbol(uuid4())
            data = DeltaArray{N + 1}(ones(size(target, first(inds))))
            push!(copies, Tensor(data, (new_index, inds...)))

            # extract diagonal of target tensor
            # TODO rewrite using `einsum!` when implemented in Tensors
            data = EinCode(
                (String.(replace(Tenet.inds(target), [i => first(inds) for i in inds[2:end]]...)),),
                String.(filter(∉(inds[2:end]), Tenet.inds(target))),
            )(
                target,
            )
            target = Tensor(
                data,
                map(index -> index === first(inds) ? new_index : index, filter(∉(inds[2:end]), Tenet.inds(target)));
            )

            return (; target = target, copies = copies)
        end

        transformed_tn = TensorNetwork(Tensor[transformed_tensor.target, transformed_tensor.copies...])
        replace!(tn, tensor => transformed_tn)
    end

    return tn
end

"""
    RankSimplification <: Transformation

Preemptively contract tensors whose result doesn't increase in size.
"""
struct RankSimplification <: Transformation end

function transform!(tn::AbstractTensorNetwork, ::RankSimplification)
    @label rank_transformation_start
    for tensor in tensors(tn)
        # TODO replace this code for `neighbours` method
        connected_tensors = mapreduce(label -> select(tn, label), ∪, inds(tensor))
        filter!(!=(tensor), connected_tensors)

        for c_tensor in connected_tensors
            # TODO keep output inds?
            path = sum([
                EinExpr(inds(tensor), Dict(index => size(tensor, index) for index in inds(tensor))) for
                tensor in [tensor, c_tensor]
            ])

            # Check if contraction does not increase the rank
            EinExprs.removedrank(path) < 0 && continue

            new_tensor = contract(tensor, c_tensor)

            # Update tensor network
            push!(tn, new_tensor)
            delete!(tn, tensor)
            delete!(tn, c_tensor)

            # Break the loop since we modified the network and need to recheck connections
            @goto rank_transformation_start
        end
    end

    return tn
end

"""
    AntiDiagonalGauging <: Transformation

Reverse the order of tensor indices that fulfill the anti-diagonal condition.
While this transformation doesn't directly enhance computational efficiency, it sets up the [`TensorNetwork`](@ref) for other operations that do.

# Keyword Arguments

  - `atol` Absolute tolerance. Defaults to `1e-12`.
  - `skip` List of indices to skip. Defaults to `[]`.
"""
Base.@kwdef struct AntiDiagonalGauging <: Transformation
    atol::Float64 = 1e-12
    skip::Vector{Symbol} = Symbol[]
end

function transform!(tn::AbstractTensorNetwork, config::AntiDiagonalGauging)
    skip_inds = isempty(config.skip) ? inds(tn, set = :open) : config.skip

    for tensor in keys(tn.tensormap)
        anti_diag_axes = find_anti_diag_axes(parent(tensor), atol = config.atol)

        for (i, j) in anti_diag_axes # loop over all anti-diagonal axes
            ix_i, ix_j = inds(tensor)[i], inds(tensor)[j]

            # do not gauge output indices
            _, ix_to_gauge = (ix_j ∈ skip_inds) ? ((ix_i ∈ skip_inds) ? continue : (ix_j, ix_i)) : (ix_i, ix_j)

            # reverse the order of ix_to_gauge in all tensors where it appears
            for t in tensors(tn)
                ix_to_gauge in inds(t) && reverse!(parent(t), dims = findfirst(l -> l == ix_to_gauge, inds(t)))
            end
        end
    end

    return tn
end

"""
    ColumnReduction <: Transformation

Truncate the dimension of a `Tensor` in a [`TensorNetwork`](@ref) when it contains columns with all elements smaller than `atol`.

# Keyword Arguments

  - `atol` Absolute tolerance. Defaults to `1e-12`.
  - `skip` List of indices to skip. Defaults to `[]`.
"""
Base.@kwdef struct ColumnReduction <: Transformation
    atol::Float64 = 1e-12
    skip::Vector{Symbol} = Symbol[]
end

function transform!(tn::AbstractTensorNetwork, config::ColumnReduction)
    skip_inds = isempty(config.skip) ? inds(tn, set = :open) : config.skip

    for tensor in tensors(tn)
        for (dim, index) in enumerate(inds(tensor))
            index ∈ skip_inds && continue

            zeroslices = iszero.(eachslice(tensor, dims = dim))
            any(zeroslices) || continue

            slice!(tn, index, count(!, zeroslices) == 1 ? findfirst(!, zeroslices) : findall(!, zeroslices))
        end
    end

    return tn
end

"""
    SplitSimplification <: Transformation

Reduce the rank of tensors in the [`TensorNetwork`](@ref) by decomposing them using the Singular Value Decomposition (SVD). Tensors whose factorization do not increase the maximum rank of the network are left decomposed.

# Keyword Arguments

  - `atol` Absolute tolerance. Defaults to `1e-10`.
"""
Base.@kwdef struct SplitSimplification <: Transformation
    atol::Float64 = 1e-10  # A threshold for SVD rank determination
end

function transform!(tn::AbstractTensorNetwork, config::SplitSimplification)
    @label split_simplification_start
    for tensor in tensors(tn)
        inds = Tenet.inds(tensor)

        # iterate all bipartitions of the tensor's indices
        bipartitions = Iterators.flatten(combinations(inds, r) for r in 1:(length(inds)-1))
        for bipartition in bipartitions
            left_inds = collect(bipartition)

            # perform an SVD across the bipartition
            u, s, v = svd(tensor; left_inds = left_inds)
            rank_s = sum(s .> config.atol)

            if rank_s < length(s)
                hyperindex = only(Tenet.inds(s))

                # truncate data
                u = view(u, hyperindex => 1:rank_s)
                s = view(s, hyperindex => 1:rank_s)
                v = view(v, hyperindex => 1:rank_s)

                # replace the original tensor with factorization
                tensor_l = contract(u, s, dims = Symbol[])
                tensor_r = v

                push!(tn, dropdims(tensor_l))
                push!(tn, dropdims(tensor_r))
                pop!(tn, tensor)

                # iterator is no longer valid, so restart loop
                @goto split_simplification_start
            end
        end
    end
    return tn
end

function find_diag_axes(x; atol = 1e-12)
    # skip 1D tensors
    ndims(parent(x)) == 1 && return []

    # find all the potential diagonals
    potential_diag_axes = [(i, j) for i in 1:ndims(x) for j in i+1:ndims(x) if size(x, i) == size(x, j)]

    # check what elements satisfy the condition
    diag_pairs = filter(potential_diag_axes) do (d1, d2)
        all(pairs(parent(x))) do (idx, val)
            idx[d1] == idx[d2] || abs(val) <= atol
        end
    end

    # if overlap between pairs of diagonal axes, then all involved axes are diagonal
    diag_sets = reduce(diag_pairs; init = Vector{Int}[]) do acc, pair
        i = findfirst(set -> !isdisjoint(set, pair), acc)
        !isnothing(i) ? union!(acc[i], pair) : push!(acc, collect(pair))
        return acc
    end

    # map to index symbols
    map(set -> map(i -> inds(x)[i], set), diag_sets)
end

function find_anti_diag_axes(x; atol = 1e-12)
    # skip 1D tensors
    ndims(parent(x)) == 1 && return []

    # Find all the potential anti-diagonals
    potential_anti_diag_axes = [(i, j) for i in 1:ndims(x) for j in i+1:ndims(x) if size(x, i) == size(x, j)]

    # Check what elements satisfy the condition
    return filter(potential_anti_diag_axes) do (d1, d2)
        all(pairs(parent(x))) do (idx, val)
            idx[d1] != size(x, d1) - idx[d2] || abs(val) <= atol
        end
    end
end
